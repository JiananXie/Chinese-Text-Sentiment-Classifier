{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "example_text = '我喜欢自然语言处理'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "bert_input = tokenizer(example_text, truncation= True, return_tensors='pt')\n",
    "print(tokenizer.encode(example_text))   \n",
    "print(tokenizer.decode(bert_input.input_ids[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd_all = pd.read_csv('ChnSentiCorp_htl_all.csv')\n",
    "pd_all.dropna(inplace= True)\n",
    "len(pd_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造平衡语料\n",
    "pd_positive = pd_all[pd_all.label==1]\n",
    "pd_negative = pd_all[pd_all.label==0]\n",
    "\n",
    "def get_balance_corpus(corpus_size, corpus_pos, corpus_neg):\n",
    "    sample_size = corpus_size // 2\n",
    "    pd_corpus_balance = pd.concat([corpus_pos.sample(sample_size, replace=corpus_pos.shape[0]<sample_size), \\\n",
    "                                   corpus_neg.sample(sample_size, replace=corpus_neg.shape[0]<sample_size)])\n",
    "    \n",
    "    print('评论数目（总体）：%d' % pd_corpus_balance.shape[0])\n",
    "    print('评论数目（正向）：%d' % pd_corpus_balance[pd_corpus_balance.label==1].shape[0])\n",
    "    print('评论数目（负向）：%d' % pd_corpus_balance[pd_corpus_balance.label==0].shape[0])    \n",
    "    \n",
    "    return pd_corpus_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChnSentiCorp_htl_ba_4800 = get_balance_corpus(4800, pd_positive, pd_negative)\n",
    "\n",
    "ChnSentiCorp_htl_ba_4800.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_train, data_valid= np.split(ChnSentiCorp_htl_ba_4800.sample(frac=1,random_state=42) ,[int(0.8*len(ChnSentiCorp_htl_ba_4800))],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = df['label'].values\n",
    "        self.reviews = [tokenizer(review, \n",
    "                                padding='max_length', \n",
    "                                max_length = 512, \n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\") \n",
    "                      for review in df['review']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_reviews(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.reviews[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_reviews = self.get_batch_reviews(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_reviews, batch_y\n",
    "\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.4):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('hfl/chinese-bert-wwm-ext')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam,AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "  # 通过Dataset类获取训练和验证集\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "    # DataLoader根据batch_size获取数据，训练时选择打乱样本\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2,shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "  # 判断是否使用GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.empty_cache()\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "    # 开始进入训练循环\n",
    "    for epoch_num in range(epochs):\n",
    "      # 定义两个变量，用于存储训练集的准确率和损失\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "      # 进度条函数tqdm    \n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "                train_label = train_label.type(torch.LongTensor).to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "        # 通过模型得到输出\n",
    "                optimizer.zero_grad()\n",
    "                output = model(input_id, mask)\n",
    "                # 计算损失\n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                # 计算精度\n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "        # 模型更新\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            # ------ 验证模型 -----------\n",
    "            # 定义两个变量，用于存储验证集的准确率和损失\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "      # 不需要计算梯度\n",
    "            with torch.no_grad():\n",
    "                # 循环获取数据集，并用训练好的模型进行验证\n",
    "                for val_input, val_label in val_dataloader:\n",
    "          # 如果有GPU，则使用GPU，接下来的操作同训练\n",
    "                    val_label = val_label.type(torch.LongTensor).to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "  \n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'''Epochs: {epoch_num + 1} \n",
    "              | Train Loss: {total_loss_train / len(train_data): .3f} \n",
    "              | Train Accuracy: {total_acc_train / len(train_data): .3f} \n",
    "              | Val Loss: {total_loss_val / len(val_data): .3f} \n",
    "              | Val Accuracy: {total_acc_val / len(val_data): .3f}''')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 2e-6\n",
    "train(model, data_train, data_valid, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取csv文件\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# 删除review字段中的空格\n",
    "df['review'] = df['review'].str.replace(' ', '')\n",
    "\n",
    "# 保存新的dataframe到csv文件\n",
    "df.to_csv('new_train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 读取json文件\n",
    "with open('train.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 将数据转换为DataFrame\n",
    "df = pd.DataFrame(data,columns=['review','label'])\n",
    "\n",
    "# 保存为csv文件\n",
    "df.to_csv('train_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# 解析XML文件\n",
    "tree = ET.parse('dataset/Sina/Training data for Emotion Classification.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# 提取数据\n",
    "data = []\n",
    "for sentence in root.iter('sentence'):\n",
    "    review = sentence.text\n",
    "    label = sentence.get('emotion-1-type')\n",
    "    if label is not None:  # 只保留有情绪标签的句子\n",
    "        data.append((review, label))\n",
    "\n",
    "# 创建dataframe\n",
    "df = pd.DataFrame(data, columns=['review', 'label'])\n",
    "df.to_csv('Sina1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# 解析XML文件\n",
    "tree = ET.parse('dataset/Sina/EmotionClassficationTest.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# 提取数据\n",
    "data = []\n",
    "for sentence in root.iter('sentence'):\n",
    "    review = sentence.text\n",
    "    label = sentence.get('emotion-1-type')\n",
    "    if label is not None:  # 只保留有情绪标签的句子\n",
    "        data.append((review, label))\n",
    "\n",
    "# 创建dataframe\n",
    "df = pd.DataFrame(data, columns=['review', 'label'])\n",
    "df.to_csv('Sina2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# 解析XML文件\n",
    "tree = ET.parse('dataset/Sina/ExpressionTest.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# 提取数据\n",
    "data = []\n",
    "for sentence in root.iter('sentence'):\n",
    "    review = sentence.text\n",
    "    label = sentence.get('emotion-1-type')\n",
    "    if label is not None:  # 只保留有情绪标签的句子\n",
    "        data.append((review, label))\n",
    "\n",
    "# 创建dataframe\n",
    "df = pd.DataFrame(data, columns=['review', 'label'])\n",
    "df.to_csv('Sina3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# 解析XML文件\n",
    "tree = ET.parse('dataset/Sina/NLPCC2014微博情绪分析样例数据.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# 提取数据\n",
    "data = []\n",
    "for sentence in root.iter('sentence'):\n",
    "    review = sentence.text\n",
    "    label = sentence.get('emotion-1-type')\n",
    "    if label is not None:  # 只保留有情绪标签的句子\n",
    "        data.append((review, label))\n",
    "\n",
    "# 创建dataframe\n",
    "df = pd.DataFrame(data, columns=['review', 'label'])\n",
    "df.to_csv('Sina4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    13993\n",
       "1     6697\n",
       "3     5978\n",
       "2     5348\n",
       "5     4950\n",
       "4     3167\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_cat6.csv')\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取四个csv文件\n",
    "df1 = pd.read_csv('Sina1.csv')\n",
    "df2 = pd.read_csv('Sina2.csv')\n",
    "df3 = pd.read_csv('Sina3.csv')\n",
    "df4 = pd.read_csv('Sina4.csv')\n",
    "\n",
    "# 合并四个dataframe\n",
    "df = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "df = df[df['label'].isin(['like', 'sadness', 'happiness', 'disgust','anger'])]\n",
    "df['label'] = df['label'].map({'like':'1','sadness':'2','disgust':'3','anger':'4','happiness':'5'})\n",
    "# 保存为csv文件\n",
    "df.to_csv('Sina.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    7781\n",
       "3    4761\n",
       "5    4720\n",
       "2    3776\n",
       "4    2895\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Sina.csv')\n",
    "df2 = pd.read_csv('train_cat6.csv')\n",
    "final_df = pd.concat([df1,df2],ignore_index=True)\n",
    "final_df.to_csv('dataset/Sina.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    14478\n",
       "0    13993\n",
       "3    10739\n",
       "5     9670\n",
       "2     9124\n",
       "4     6062\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取ChnSentiCorp_htl_all.csv文件\n",
    "df1 = pd.read_csv('dataset/ChnSentiCorp_htl_all.csv')\n",
    "df1['label'] = df1['label'].apply(lambda x: -1 if x == 0 else x)\n",
    "\n",
    "# 读取Sina.csv文件\n",
    "df2 = pd.read_csv('dataset/Sina.csv')\n",
    "df2['label'] = df2['label'].apply(lambda x: 1 if x == 5 else (-1 if x in [2, 3, 4] else x))\n",
    "\n",
    "# 拼接两个dataframe\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "df['label'] = df['label'] + 1\n",
    "# 保存为csv文件\n",
    "df.to_csv('dataset/Positive_null_negative.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    29470\n",
       "0    28369\n",
       "1    13993\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/Positive_null_negative.csv')\n",
    "\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40133"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/Weibo.csv')\n",
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
